{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install librosa pretty_midi\n",
        "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
        "!pip install miditok\n",
        "!pip install tqdm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m-3zEqFXm7fM",
        "outputId": "9375fa9d-b53e-403b-a1ae-20d279842b58"
      },
      "id": "m-3zEqFXm7fM",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Collecting pretty_midi\n",
            "  Downloading pretty_midi-0.2.10.tar.gz (5.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m106.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.13.2)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.0)\n",
            "Collecting mido>=1.1.16 (from pretty_midi)\n",
            "  Downloading mido-1.3.3-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.11/dist-packages (from pretty_midi) (1.17.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.4.26)\n",
            "Downloading mido-1.3.3-py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.6/54.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pretty_midi\n",
            "  Building wheel for pretty_midi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pretty_midi: filename=pretty_midi-0.2.10-py3-none-any.whl size=5592286 sha256=96fcc95ebef58abb918402c6f4641e8bca13fce722e5c31cae126c81e620eeb9\n",
            "  Stored in directory: /root/.cache/pip/wheels/e6/95/ac/15ceaeb2823b04d8e638fd1495357adb8d26c00ccac9d7782e\n",
            "Successfully built pretty_midi\n",
            "Installing collected packages: mido, pretty_midi\n",
            "Successfully installed mido-1.3.3 pretty_midi-0.2.10\n",
            "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "INFO: pip is looking at multiple versions of torch to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.7.1%2Bcu118-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (28 kB)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
            "Collecting nvidia-cuda-nvrtc-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_nvrtc_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (23.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu11==11.8.89 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_runtime_cu11-11.8.89-py3-none-manylinux1_x86_64.whl (875 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m875.6/875.6 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu11==11.8.87 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cuda_cupti_cu11-11.8.87-py3-none-manylinux1_x86_64.whl (13.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m122.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu11==9.1.0.70 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cudnn_cu11-9.1.0.70-py3-none-manylinux2014_x86_64.whl (663.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.9/663.9 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu11==11.11.3.6 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cublas_cu11-11.11.3.6-py3-none-manylinux1_x86_64.whl (417.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m417.9/417.9 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu11==10.9.0.58 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cufft_cu11-10.9.0.58-py3-none-manylinux1_x86_64.whl (168.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m168.4/168.4 MB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu11==10.3.0.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_curand_cu11-10.3.0.86-py3-none-manylinux1_x86_64.whl (58.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu11==11.4.1.48 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusolver_cu11-11.4.1.48-py3-none-manylinux1_x86_64.whl (128.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m128.2/128.2 MB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu11==11.7.5.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_cusparse_cu11-11.7.5.86-py3-none-manylinux1_x86_64.whl (204.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m204.1/204.1 MB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu11==2.21.5 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nccl_cu11-2.21.5-py3-none-manylinux2014_x86_64.whl (147.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m147.8/147.8 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu11==11.8.86 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/cu118/nvidia_nvtx_cu11-11.8.86-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting triton==3.3.1 (from torch)\n",
            "  Downloading https://download.pytorch.org/whl/triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Collecting torch\n",
            "  Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl.metadata (27 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading https://download.pytorch.org/whl/cu118/torch-2.6.0%2Bcu118-cp311-cp311-linux_x86_64.whl (848.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m848.7/848.7 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvtx-cu11, nvidia-nccl-cu11, nvidia-cusparse-cu11, nvidia-curand-cu11, nvidia-cufft-cu11, nvidia-cuda-runtime-cu11, nvidia-cuda-nvrtc-cu11, nvidia-cuda-cupti-cu11, nvidia-cublas-cu11, nvidia-cusolver-cu11, nvidia-cudnn-cu11, torch\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.6.0+cu124\n",
            "    Uninstalling torch-2.6.0+cu124:\n",
            "      Successfully uninstalled torch-2.6.0+cu124\n",
            "Successfully installed nvidia-cublas-cu11-11.11.3.6 nvidia-cuda-cupti-cu11-11.8.87 nvidia-cuda-nvrtc-cu11-11.8.89 nvidia-cuda-runtime-cu11-11.8.89 nvidia-cudnn-cu11-9.1.0.70 nvidia-cufft-cu11-10.9.0.58 nvidia-curand-cu11-10.3.0.86 nvidia-cusolver-cu11-11.4.1.48 nvidia-cusparse-cu11-11.7.5.86 nvidia-nccl-cu11-2.21.5 nvidia-nvtx-cu11-11.8.86 torch-2.6.0+cu118\n",
            "Collecting miditok\n",
            "  Downloading miditok-3.0.5.post1-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from miditok) (0.32.2)\n",
            "Requirement already satisfied: numpy>=1.19 in /usr/local/lib/python3.11/dist-packages (from miditok) (2.0.2)\n",
            "Collecting symusic>=0.5.0 (from miditok)\n",
            "  Downloading symusic-0.5.8-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: tokenizers>=0.13.0 in /usr/local/lib/python3.11/dist-packages (from miditok) (0.21.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from miditok) (4.67.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (2.32.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (4.13.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.16.4->miditok) (1.1.2)\n",
            "Collecting pySmartDL (from symusic>=0.5.0->miditok)\n",
            "  Downloading pySmartDL-1.3.4-py3-none-any.whl.metadata (2.8 kB)\n",
            "Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from symusic>=0.5.0->miditok) (4.3.8)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.16.4->miditok) (2025.4.26)\n",
            "Downloading miditok-3.0.5.post1-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m16.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading symusic-0.5.8-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pySmartDL-1.3.4-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: pySmartDL, symusic, miditok\n",
            "Successfully installed miditok-3.0.5.post1 pySmartDL-1.3.4 symusic-0.5.8\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import librosa\n",
        "import pretty_midi\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import miditok\n",
        "from miditok import TokSequence\n",
        "from tqdm.auto import tqdm"
      ],
      "metadata": {
        "id": "eFzQ6PKBn4Mi"
      },
      "id": "eFzQ6PKBn4Mi",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Using device:\", device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a26Z555WXCBo",
        "outputId": "89e5f884-88b3-4e88-a57e-d0bd5c8eed4a"
      },
      "id": "a26Z555WXCBo",
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TOKENIZER_PARAMS = {\n",
        "\"pitch_range\": (20, 110),\n",
        "\"beat_res\": {(0,4):24, (4,12):12},\n",
        "\"special_tokens\": [\"PAD\", \"BOS\", \"EOS\", \"MASK\"],\n",
        "\"use_chords\": True,\n",
        "\"use_rests\": False,\n",
        "\"use_tempos\": True,\n",
        "\"use_time_signatures\": True,\n",
        "\"use_programs\": False,\n",
        "\"use_microtiming\": False,\n",
        "\"ticks_per_quarter\": 220,\n",
        "\"max_microtiming_shift\": 0.125,\n",
        "\"num_microtiming_bins\": 30,\n",
        "}\n",
        "config = miditok.TokenizerConfig(**TOKENIZER_PARAMS)\n",
        "\n",
        "tokenizer=miditok.PerTok(config)"
      ],
      "metadata": {
        "id": "JyHxZbnQyI0j"
      },
      "id": "JyHxZbnQyI0j",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "DATA_ROOT='/content/drive/My Drive/song2cover'\n",
        "PAD_TOKEN = tokenizer.pad_token_id\n",
        "EOS_TOKEN = tokenizer.special_tokens_ids[2]\n",
        "VocabSize = tokenizer.vocab_size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8JMxD4OPoFSQ",
        "outputId": "318db9b7-5738-418e-d5b9-a9443712a367"
      },
      "id": "8JMxD4OPoFSQ",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def collate_fn(batch):\n",
        "  #xs, ys, zs, a, r = zip(*batch)\n",
        "  xs, ys = zip(*batch)\n",
        "\n",
        "\n",
        "\n",
        "  y_max_len = max([y.shape[0] for y in ys])\n",
        "\n",
        "  x_max_len = max([x.shape[2] for x in xs])\n",
        "\n",
        "  y_padded = []\n",
        "  for y in ys:\n",
        "    pad_amt = y_max_len - y.shape[0]\n",
        "    padded = F.pad(y, (0, pad_amt), value=PAD_TOKEN)\n",
        "    y_padded.append(padded)\n",
        "\n",
        "  x_padded = []\n",
        "  for x in xs:\n",
        "    pad_amt = x_max_len - x.shape[2]\n",
        "    padded = F.pad(x, (0, pad_amt), value=0)\n",
        "    x_padded.append(padded)\n",
        "\n",
        "  x_batch = torch.stack(x_padded, dim=0)\n",
        "  y_batch = torch.stack(y_padded, dim=0)\n",
        "  #z_batch = torch.stack(zs, dim=0)\n",
        "  #a_batch = torch.stack(a, dim=0)\n",
        "  #r_batch = torch.stack(r, dim=0)\n",
        "\n",
        "  #return x_batch, y_batch, z_batch, a_batch, r_batch\n",
        "  return x_batch, y_batch"
      ],
      "metadata": {
        "id": "VwsDRw_rInd3"
      },
      "id": "VwsDRw_rInd3",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioMidiDataset(Dataset):\n",
        "  def __init__(self, root_dir, tokenizer, transform=None):\n",
        "    self.root_dir = root_dir\n",
        "    self.transform = transform\n",
        "    df = pd.read_csv(os.path.join(root_dir, 'manifest.csv'))\n",
        "    self.items = df.to_dict('records')\n",
        "    self.tokenizer = tokenizer\n",
        "\n",
        "  def __len__(self):\n",
        "    return len(self.items)\n",
        "\n",
        "  def __getitem__(self, idx):\n",
        "    item = self.items[idx]\n",
        "    audio_path = os.path.join(self.root_dir, item['audio_filepath'])\n",
        "    midi_path = os.path.join(self.root_dir, item['midi_filepath'])\n",
        "    #diff = torch.tensor(float(item['difficulty'])).float()\n",
        "    #pos_rel = torch.tensor(float(item['pos_percent'])).float()\n",
        "    #pos_abs = torch.tensor(float(item['pos_absolute']), dtype=torch.long)\n",
        "\n",
        "    y, sr = librosa.load(audio_path)\n",
        "    stft = librosa.stft(y, n_fft=1024, hop_length=256)\n",
        "    log_stft = librosa.amplitude_to_db(np.abs(stft), ref=np.max)\n",
        "    x = torch.tensor(log_stft).unsqueeze(0).float()\n",
        "\n",
        "    tokens = self.tokenizer.encode(midi_path)\n",
        "    y_tokens = torch.flatten(torch.tensor(tokens, dtype=torch.long))\n",
        "\n",
        "    eos = torch.tensor([self.tokenizer.special_tokens_ids[2]], dtype=torch.long)\n",
        "    y_tokens = torch.cat((y_tokens, eos))\n",
        "\n",
        "    sample = {\n",
        "        'x': x,\n",
        "        'y': y_tokens,\n",
        "        #'diff': diff,\n",
        "        #'pos_percent': pos_rel,\n",
        "        #'pos_absolute': pos_abs,\n",
        "    }\n",
        "\n",
        "    if self.transform:\n",
        "      sample = self.transform(sample)\n",
        "\n",
        "    #return x, y_tokens, diff, pos_abs, pos_rel\n",
        "    return x, y_tokens"
      ],
      "metadata": {
        "id": "6t5cAftuoc8z"
      },
      "id": "6t5cAftuoc8z",
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = AudioMidiDataset(DATA_ROOT, tokenizer)\n",
        "\n",
        "loader = DataLoader(\n",
        "    dataset,\n",
        "    batch_size=16,\n",
        "    shuffle=True,\n",
        "    num_workers=0,\n",
        "    collate_fn=collate_fn\n",
        ")"
      ],
      "metadata": {
        "id": "SR4dHBmcLfom"
      },
      "id": "SR4dHBmcLfom",
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioEncoder(nn.Module):\n",
        "  def __init__(self, d_model=512):\n",
        "    super().__init__()\n",
        "\n",
        "    self.conv = nn.Sequential(\n",
        "        nn.Conv2d(1, 64, kernel_size=3, stride=(2,2), padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(64, 128, kernel_size=3, stride=(2,2), padding=1),\n",
        "        nn.ReLU(),\n",
        "        nn.Conv2d(128, d_model, kernel_size=3, stride=(2,2), padding=1),\n",
        "        nn.ReLU()\n",
        "    )\n",
        "\n",
        "    self.project = nn.Linear(in_features=d_model * 65, out_features=d_model)\n",
        "    self.pos_enc = nn.Embedding(num_embeddings=1500, embedding_dim=d_model)\n",
        "\n",
        "    enc_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=8, dim_feedforward=d_model * 4)\n",
        "    self.encoder = nn.TransformerEncoder(enc_layer, num_layers=3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    z = self.conv(x)\n",
        "\n",
        "    B, D, F_enc, T_enc = z.shape\n",
        "\n",
        "    enc_in = z.permute(0, 3, 1, 2).reshape(B, T_enc, D * F_enc)\n",
        "    enc_proj = self.project(enc_in)\n",
        "\n",
        "    positions = torch.arange(T_enc, device=x.device).unsqueeze(0)\n",
        "    enc_pos = enc_proj + self.pos_enc(positions)\n",
        "\n",
        "    enc_in_tp = enc_pos.permute(1,0,2)\n",
        "    enc_out_tp = self.encoder(enc_in_tp)\n",
        "\n",
        "    enc_out = enc_out_tp.permute(1,0,2)\n",
        "\n",
        "    return enc_out\n",
        "\n",
        "class TokenDecoder(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model=512, max_len=1500, max_measures=200,):\n",
        "    super().__init__()\n",
        "\n",
        "\n",
        "    self.d_model = d_model\n",
        "\n",
        "    self.token_emb = nn.Embedding(num_embeddings=vocab_size, embedding_dim=d_model)\n",
        "    self.pos_emb = nn.Embedding(num_embeddings=max_len, embedding_dim=d_model)\n",
        "    #self.measure_emb = nn.Embedding(num_embeddings=max_measures, embedding_dim=d_model)\n",
        "    #self.diff_proj = nn.Sequential(\n",
        "    #    nn.Linear(in_features=1, out_features=d_model),\n",
        "    #    nn.ReLU(),\n",
        "    #    nn.Linear(d_model, d_model)\n",
        "    #)\n",
        "\n",
        "    #self.rel_proj = nn.Sequential(\n",
        "    #    nn.Linear(in_features=1, out_features=d_model),\n",
        "    #    nn.ReLU(),\n",
        "    #   nn.Linear(d_model, d_model)\n",
        "    #)\n",
        "\n",
        "    dec_layer = nn.TransformerDecoderLayer(d_model=d_model, nhead=8, dim_feedforward=d_model * 4)\n",
        "    self.decoder = nn.TransformerDecoder(dec_layer, num_layers=3)\n",
        "\n",
        "    self.output_fc = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "  #def forward(self, y_input, memory, diff, pos_abs, pos_rel):\n",
        "  def forward(self, y_input, memory):\n",
        "    B, T_tgt = y_input.shape\n",
        "\n",
        "    tok_embed = self.token_emb(y_input)\n",
        "\n",
        "    pos = torch.arange(T_tgt, device = y_input.device).unsqueeze(0)\n",
        "    pos_embed = self.pos_emb(pos)\n",
        "\n",
        "    #measure_emb = self.measure_emb(pos_abs)\n",
        "    #measure_b = measure_emb.unsqueeze(1).expand(-1, T_tgt, -1)\n",
        "\n",
        "    #diff = diff.unsqueeze(-1)\n",
        "    #diff_emb = self.diff_proj(diff)\n",
        "    #diff_b = diff_emb.unsqueeze(1).expand(-1, T_tgt, -1)\n",
        "\n",
        "    #rel = pos_rel.unsqueeze(-1)\n",
        "    #rel_emb = self.rel_proj(rel)\n",
        "    #rel_b = rel_emb.unsqueeze(1).expand(-1, T_tgt, -1)\n",
        "\n",
        "    #tgt = tok_embed + pos_embed + measure_b + diff_b + rel_b\n",
        "    tgt = tok_embed + pos_embed\n",
        "\n",
        "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(T_tgt).to(y_input.device)\n",
        "\n",
        "    tgt2 = tgt.permute(1,0,2)\n",
        "    mem2 = memory.permute(1,0,2)\n",
        "\n",
        "    dec_out2 = self.decoder(tgt2, mem2, tgt_mask=tgt_mask)\n",
        "\n",
        "    dec_out = dec_out2.permute(1,0,2)\n",
        "\n",
        "    logits = self.output_fc(dec_out)\n",
        "\n",
        "    return logits\n",
        "\n",
        "\n",
        "class Audio2Midi(nn.Module):\n",
        "  def __init__(self, vocab_size, d_model=512, max_tok_len=1500):\n",
        "    super().__init__()\n",
        "    self.encoder = AudioEncoder(d_model=d_model)\n",
        "    self.decoder = TokenDecoder(vocab_size, d_model=d_model, max_len=max_tok_len)\n",
        "\n",
        "  #def forward(self, x_audio, y_input, diff, pos_abs, pos_rel):\n",
        "  def forward(self, x_audio, y_input):\n",
        "    memory = self.encoder(x_audio)\n",
        "    #logits = self.decoder(y_input, memory, diff, pos_abs, pos_rel)\n",
        "    logits = self.decoder(y_input, memory)\n",
        "\n",
        "    return logits\n",
        "\n"
      ],
      "metadata": {
        "id": "EW31ncxMOsfZ"
      },
      "id": "EW31ncxMOsfZ",
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add `%load_ext cudf.pandas` before importing pandas to speed up operations using GPU"
      ],
      "metadata": {
        "id": "SlH-gSFjs2Uc"
      },
      "id": "SlH-gSFjs2Uc"
    },
    {
      "cell_type": "code",
      "source": [
        "model = Audio2Midi(vocab_size=VocabSize, d_model=512, max_tok_len=1500).to(device)\n",
        "optim = torch.optim.AdamW(model.parameters(), lr=1e-4)\n",
        "scaler = torch.amp.GradScaler('cuda')\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN)\n",
        "\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(1, num_epochs+1):\n",
        "  model.train()\n",
        "  total_loss = 0.0\n",
        "\n",
        "  batch_iterator = tqdm(loader, desc=f\"Epoch {epoch:02d}\", leave=False, bar_format='{l_bar}{bar} | {percentage:3.0f}%')\n",
        "\n",
        "  #for x_batch, y_batch, diff_batch, abs_batch, rel_batch in batch_iterator:\n",
        "  for x_batch, y_batch in batch_iterator:\n",
        "      x_batch = x_batch.to(device)\n",
        "      y_batch = y_batch.to(device)\n",
        "      #diff_batch = diff_batch.to(device)\n",
        "      #abs_batch = abs_batch.to(device)\n",
        "      #rel_batch = rel_batch.to(device)\n",
        "\n",
        "      y_input  = y_batch[:, :-1]\n",
        "      y_target = y_batch[:, 1:]\n",
        "\n",
        "      optim.zero_grad()\n",
        "\n",
        "      with torch.amp.autocast('cuda'):\n",
        "        #logits = model(x_batch, y_input, diff_batch, abs_batch, rel_batch)\n",
        "        logits = model(x_batch, y_input)\n",
        "\n",
        "        B, Tm, V = logits.shape\n",
        "        loss = criterion(\n",
        "            logits.reshape(-1, V),\n",
        "            y_target.reshape(-1),\n",
        "        )\n",
        "\n",
        "      scaler.scale(loss).backward()\n",
        "      scaler.step(optim)\n",
        "      scaler.update()\n",
        "\n",
        "      total_loss += loss.item()\n",
        "      batch_iterator.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "  avg_loss = total_loss / len(loader)\n",
        "  print(f\"Epoch {epoch:02d} — Loss {avg_loss:.4f}\")\n",
        "\n",
        "torch.save({\n",
        "  'epoch': epoch,\n",
        "  'model_state_dict': model.state_dict(),\n",
        "  'optimizer_state_dict': optim.state_dict()}, os.path.join(DATA_ROOT, 'checkpoint.pt'))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105,
          "referenced_widgets": [
            "e9f9e239df6244e7ad7149fc217a07cc",
            "7b5dc609b592497c83b0720b5794425a",
            "11777dd1ac5747f3afd1fe9c869f0f09",
            "01d1c0b3aa4e4b77a5ee2af08b54b8fa",
            "f4f142dfc8a04dcaa88bab5573c965d8",
            "42642ca7ea0f456f9c71502a9d295cc3",
            "3672330a92114933978732bf822f0815",
            "96921cdf376a46ae9ef80b8524981738",
            "ba52a15e6d9146548d34336b289aeedf",
            "d730b35b2746457185786429dccb0310",
            "83579891a63e42b48a40b89192e0e791"
          ]
        },
        "id": "x0Mew77eVych",
        "outputId": "48e2f558-52a5-4250-8c0c-f916767b4b76"
      },
      "id": "x0Mew77eVych",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/modules/transformer.py:385: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Epoch 01:   0%|           |   0%"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e9f9e239df6244e7ad7149fc217a07cc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), os.path.join(DATA_ROOT, 'model.pt'))"
      ],
      "metadata": {
        "id": "v0EJmEk--84x"
      },
      "id": "v0EJmEk--84x",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Audio2Midi(vocab_size=VocabSize, d_model=512, max_tok_len=1500).to(device)\n",
        "checkpoint = torch.load(os.path.join(DATA_ROOT, 'checkpoint.pt'), map_location=torch.device('cpu'))\n",
        "model.load_state_dict(checkpoint['model_state_dict'])"
      ],
      "metadata": {
        "id": "__8yLBh7S_Gs",
        "outputId": "4c38b0cf-b8e0-40cd-ae64-5a01913f2b8e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "__8yLBh7S_Gs",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(os.path.join(DATA_ROOT, 'model.pt'), map_location=device))"
      ],
      "metadata": {
        "id": "9YzrgZn98bIv",
        "outputId": "bfebae5b-0022-4380-d6f8-284eb3cf319b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "9YzrgZn98bIv",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#def generate_midi(model, tokenizer, audio_path, diff, pos_abs, pos_rel, max_len=1300, device='cpu', temp = 1.0):\n",
        "def generate_midi(model, tokenizer, audio_path, max_len=1500, device='cpu', temp = 1.0):\n",
        "  model.eval()\n",
        "\n",
        "  y, sr = librosa.load(audio_path)\n",
        "  stft = librosa.stft(y, n_fft=1024, hop_length=256)\n",
        "  log_stft = librosa.amplitude_to_db(np.abs(stft), ref=np.max)\n",
        "  x = torch.tensor(log_stft, dtype=torch.float32).unsqueeze(0).unsqueeze(0).to(device)\n",
        "\n",
        "  #diff = torch.tensor([diff]).float().to(device)\n",
        "  #pos_rel = torch.tensor([pos_rel]).float().to(device)\n",
        "  #pos_abs = torch.tensor([pos_abs], dtype=torch.long).to(device)\n",
        "\n",
        "\n",
        "  with torch.no_grad():\n",
        "    memory = model.encoder(x)\n",
        "\n",
        "  generated = [tokenizer.special_tokens[1]]\n",
        "  for _ in range(max_len):\n",
        "    if _ % 100 == 0:\n",
        "      print(_)\n",
        "    y_input = torch.tensor([tokenizer.vocab[t] for t in generated], dtype=torch.long).unsqueeze(0).to(device)\n",
        "    with torch.no_grad():\n",
        "      #logits = model.decoder(y_input, memory, diff, pos_abs, pos_rel)\n",
        "      logits = model.decoder(y_input, memory)\n",
        "    next_logits = logits[:, -1, :]\n",
        "    probabilities = F.softmax(next_logits / temp, dim=-1)\n",
        "    next_id = torch.multinomial(probabilities, num_samples=1).squeeze(0).item()\n",
        "    next_token = [key for key, val in tokenizer.vocab.items() if val == next_id][0]\n",
        "    generated.append(next_token)\n",
        "    if next_id == tokenizer.special_tokens_ids[2]:\n",
        "      break\n",
        "\n",
        "  if generated[0] == tokenizer.special_tokens[1]:\n",
        "    generated = generated[1:]\n",
        "  if generated[-1] == tokenizer.special_tokens[2]:\n",
        "    generated = generated[:-1]\n",
        "  generated_seq = TokSequence(tokens=generated)\n",
        "  print(len(generated_seq))\n",
        "\n",
        "  tokens = tokenizer.decode([generated_seq])\n",
        "  return tokens\n"
      ],
      "metadata": {
        "id": "uLGzalFJ-4Rr"
      },
      "id": "uLGzalFJ-4Rr",
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "path = os.path.join(DATA_ROOT,\"song\", \"0_0_0.mp3\")\n",
        "\n",
        "\n",
        "tokens = generate_midi(model, tokenizer, path, max_len=1500, device=device, temp=0.9)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDAq8sK-BhXn",
        "outputId": "8e1cffb7-9bfb-4f97-98d2-dd52d17e3fac"
      },
      "id": "qDAq8sK-BhXn",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "100\n",
            "200\n",
            "300\n",
            "400\n",
            "500\n",
            "600\n",
            "700\n",
            "800\n",
            "900\n",
            "1000\n",
            "1053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(tokens)\n",
        "\n",
        "tokens.dump_midi(os.path.join(DATA_ROOT, '1.mid'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOahP0ajK-8h",
        "outputId": "7ba4cf0d-f0be-4168-da06-40d9b1af1d92"
      },
      "id": "uOahP0ajK-8h",
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score(ttype=Tick, tpq=220, begin=0, end=13904, tracks=1, notes=286, time_sig=1, key_sig=0, markers=0)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "e9f9e239df6244e7ad7149fc217a07cc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7b5dc609b592497c83b0720b5794425a",
              "IPY_MODEL_11777dd1ac5747f3afd1fe9c869f0f09",
              "IPY_MODEL_01d1c0b3aa4e4b77a5ee2af08b54b8fa"
            ],
            "layout": "IPY_MODEL_f4f142dfc8a04dcaa88bab5573c965d8"
          }
        },
        "7b5dc609b592497c83b0720b5794425a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42642ca7ea0f456f9c71502a9d295cc3",
            "placeholder": "​",
            "style": "IPY_MODEL_3672330a92114933978732bf822f0815",
            "value": "Epoch 01:   0%"
          }
        },
        "11777dd1ac5747f3afd1fe9c869f0f09": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_96921cdf376a46ae9ef80b8524981738",
            "max": 468,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ba52a15e6d9146548d34336b289aeedf",
            "value": 0
          }
        },
        "01d1c0b3aa4e4b77a5ee2af08b54b8fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d730b35b2746457185786429dccb0310",
            "placeholder": "​",
            "style": "IPY_MODEL_83579891a63e42b48a40b89192e0e791",
            "value": " |   0%"
          }
        },
        "f4f142dfc8a04dcaa88bab5573c965d8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42642ca7ea0f456f9c71502a9d295cc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3672330a92114933978732bf822f0815": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "96921cdf376a46ae9ef80b8524981738": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ba52a15e6d9146548d34336b289aeedf": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d730b35b2746457185786429dccb0310": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "83579891a63e42b48a40b89192e0e791": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}